



@article{kotyan2022adversarial,
  title={Adversarial robustness assessment: Why in evaluation both L0 and L∞ attacks are necessary},
  author={Kotyan, Shashank and Vargas, Danilo Vasconcellos},
  journal={PloS one},
  volume={17},
  number={4},
  pages={e0265723},
  year={2022},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{hadj2018continuation,
  title={Continuation of {Nesterov}’s smoothing for regression with structured sparsity in high-dimensional neuroimaging},
  author={Hadj-Selem, Fouad and L{\"o}fstedt, Tommy and Dohmatob, Elvis and Frouin, Vincent and Dubois, Mathieu and Guillemot, Vincent and Duchesnay, Edouard},
  journal={IEEE Transactions on Medical Imaging},
  volume={37},
  number={11},
  pages={2403--2413},
  year={2018},
  publisher={IEEE}
}

 @inbook{hoffstein_pipher_silverman_2010, title={Complexity Theory and P vs NP}, booktitle={An introduction to mathematical cryptography}, publisher={Springer}, author={Hoffstein, Jeffrey and Pipher, Jill and Silverman, Joseph H.}, year={2010}, chapter={4}, pages={258-262}} 

_)


 @book{IEC61508, edition={2nd}, title={IEC 61508 Safety and Functional Safety}, publisher={International Electrotechnical Commission}, author={International Electrotechnical Commission}, year={2010}} 
 
 @book{IEC62034, edition={2nd}, title={IEC 62304 Medical Device Software - Software Life Cycle Processes}, publisher={International Electrotechnical Commission}, author={International Electrotechnical Commission}, year={2006}} 

@article{bect_bayesian_2017,
	title = {Bayesian subset simulation},
	volume = {5},
	issn = {2166-2525},
	url = {http://arxiv.org/abs/1601.02557},
	doi = {10.1137/16M1078276},
	abstract = {We consider the problem of estimating a probability of failure α, deﬁned as the volume of the excursion set of a function f : X ⊆ Rd → R above a given threshold, under a given probability measure on X. In this article, we combine the popular subset simulation algorithm (Au and Beck, Probab. Eng. Mech. 2001) and our sequential Bayesian approach for the estimation of a probability of failure (Bect, Ginsbourger, Li, Picheny and Vazquez, Stat. Comput. 2012). This makes it possible to estimate α when the number of evaluations of f is very limited and α is very small. The resulting algorithm is called Bayesian subset simulation (BSS). A key idea, as in the subset simulation algorithm, is to estimate the probabilities of a sequence of excursion sets of f above intermediate thresholds, using a sequential Monte Carlo (SMC) approach. A Gaussian process prior on f is used to deﬁne the sequence of densities targeted by the SMC algorithm, and drive the selection of evaluation points of f to estimate the intermediate probabilities. Adaptive procedures are proposed to determine the intermediate thresholds and the number of evaluations to be carried out at each stage of the algorithm. Numerical experiments illustrate that BSS achieves signiﬁcant savings in the number of function evaluations with respect to other Monte Carlo approaches.},
	language = {en},
	number = {1},
	urldate = {2020-07-20},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Bect, Julien and Li, Ling and Vazquez, Emmanuel},
	month = jan,
	year = {2017},
	note = {arXiv:1601.02557},
	keywords = {Statistics - Computation},
	pages = {762--786},
	file = {Bect et al. - 2017 - Bayesian subset simulation.pdf:C\:\\Users\\charlie\\Zotero\\storage\\CFEQS6UF\\Bect et al. - 2017 - Bayesian subset simulation.pdf:application/pdf}
}

@article{imageenhance,
  title={Relationship between entropy and SNR changes in image enhancement},
  author={Krbcova, Zuzana and Kukal, Jaromir},
  journal={EURASIP Journal on Image and Video Processing},
  volume={2017},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Springer}
}
@article{chung1988labelings,
  title={Labelings of graphs},
  author={Chung, Fan RK},
  journal={Selected topics in graph theory},
  volume={3},
  pages={151--168},
  year={1988}
}

@inproceedings{hamming,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@inproceedings{testautomation,
  title={Test automation for safety-critical systems: Industrial application and future developments},
  author={Peleska, Jan},
  booktitle={International Symposium of Formal Methods Europe},
  pages={39--59},
  year={1996},
  organization={Springer}
}

@article{vapnik1994measuring,
  title={Measuring the VC-dimension of a learning machine},
  author={Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},
  journal={Neural computation},
  volume={6},
  number={5},
  pages={851--876},
  year={1994},
  publisher={MIT Press}
}


@inproceedings{ahn2003captcha,
  title={CAPTCHA: Using hard AI problems for security},
  author={Ahn, Luis von and Blum, Manuel and Hopper, Nicholas J and Langford, John},
  booktitle={International conference on the theory and applications of cryptographic techniques},
  pages={294--311},
  year={2003},
  organization={Springer}
}

@book{aviation,
  title={Developing safety-critical software: a practical guide for aviation software and DO-178C compliance},
  author={Rierson, Leanna},
  year={2017},
  publisher={CRC Press}
}

@article{ma2020imbalanced,
  title={Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness},
  author={Ma, Xingjun and Jiang, Linxi and Huang, Hanxun and Weng, Zejia and Bailey, James and Jiang, Yu-Gang},
  journal={arXiv:2006.13726},
  year={2020}
}


@article{chakraborty2018adversarial,
  title={Adversarial attacks and defences: A survey},
  author={Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  journal={arXiv:1810.00069},
  year={2018}
}

@inproceedings{braking,
  title={Autonomous braking system via deep reinforcement learning},
  author={Hyunmin Chae and Chang Mook Kang and ByeoungDo Kim and Jaekyum Kim and Chung Choo Chung and Jun Won Choi},
  booktitle={{IEEE} 20th International conference on intelligent transportation systems ({ITSC})},
  year={2017},
}

@inproceedings{cintas_detecting_2020,
	address = {Yokohama, Japan},
	title = {Detecting {Adversarial} {Attacks} via {Subset} {Scanning} of {Autoencoder} {Activations} and {Reconstruction} {Error}},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/122},
	doi = {10.24963/ijcai.2020/122},
	abstract = {Reliably detecting attacks in a given set of inputs is of high practical relevance because of the vulnerability of neural networks to adversarial examples. These altered inputs create a security risk in applications with real-world consequences, such as self-driving cars, robotics and ﬁnancial services. We propose an unsupervised method for detecting adversarial attacks in inner layers of autoencoder (AE) networks by maximizing a non-parametric measure of anomalous node activations. Previous work in this space has shown AE networks can detect anomalous images by thresholding the reconstruction error produced by the ﬁnal layer. Furthermore, other detection methods rely on data augmentation or specialized training techniques which must be asserted before training time. In contrast, we use subset scanning methods from the anomalous pattern detection domain to enhance detection power without labeled examples of the noise, retraining or data augmentation methods. In addition to an anomalous “score” our proposed method also returns the subset of nodes within the AE network that contributed to that score. This will allow future work to pivot from detection to visualisation and explainability. Our scanning approach shows consistently higher detection power than existing detection methods across several adversarial noise models and a wide range of perturbation strengths.},
	language = {en},
	urldate = {2020-11-03},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Cintas, Celia and Speakman, Skyler and Akinwande, Victor and Ogallo, William and Weldemariam, Komminist and Sridharan, Srihari and McFowland, Edward},
	month = jul,
	year = {2020},
	pages = {876--882},
	file = {Cintas et al. - 2020 - Detecting Adversarial Attacks via Subset Scanning .pdf:C\:\\Users\\charlie\\Zotero\\storage\\RU2HGKV3\\Cintas et al. - 2020 - Detecting Adversarial Attacks via Subset Scanning .pdf:application/pdf}
}

@article{croce_reliable_2020,
	title = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	url = {http://arxiv.org/abs/2003.01690},
	abstract = {The ﬁeld of defense strategies against adversarial attacks has signiﬁcantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufﬁcient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difﬁcult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we ﬁrst propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:2003.01690 [cs, stat]},
	author = {Croce, Francesco and Hein, Matthias},
	month = aug,
	year = {2020},
	note = {arXiv:2003.01690},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Croce and Hein - 2020 - Reliable evaluation of adversarial robustness with.pdf:C\:\\Users\\charlie\\Zotero\\storage\\R7F4DQN8\\Croce and Hein - 2020 - Reliable evaluation of adversarial robustness with.pdf:application/pdf}
}


@article{paudice_detection_2018,
	title = {Detection of {Adversarial} {Training} {Examples} in {Poisoning} {Attacks} through {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1802.03041},
	abstract = {Machine learning has become an important component for many systems and applications including computer vision, spam ﬁltering, malware and network intrusion detection, among others. Despite the capabilities of machine learning algorithms to extract valuable information from data and produce accurate predictions, it has been shown that these algorithms are vulnerable to attacks. Data poisoning is one of the most relevant security threats against machine learning systems, where attackers can subvert the learning process by injecting malicious samples in the training data. Recent work in adversarial machine learning has shown that the so-called optimal attack strategies can successfully poison linear classiﬁers, degrading the performance of the system dramatically after compromising a small fraction of the training dataset. In this paper we propose a defence mechanism to mitigate the effect of these optimal poisoning attacks based on outlier detection. We show empirically that the adversarial examples generated by these attack strategies are quite different from genuine points, as no detectability constrains are considered to craft the attack. Hence, they can be detected with an appropriate pre-ﬁltering of the training dataset.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1802.03041 [cs, stat]},
	author = {Paudice, Andrea and Muñoz-González, Luis and Gyorgy, Andras and Lupu, Emil C.},
	month = feb,
	year = {2018},
	note = {arXiv:1802.03041},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Paudice et al. - 2018 - Detection of Adversarial Training Examples in Pois.pdf:C\:\\Users\\charlie\\Zotero\\storage\\ABWQPGBP\\Paudice et al. - 2018 - Detection of Adversarial Training Examples in Pois.pdf:application/pdf}
}


@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	note = {arXiv:1511.04508},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}


@article{daya_graph-based_2019,
	title = {A {Graph}-{Based} {Machine} {Learning} {Approach} for {Bot} {Detection}},
	url = {http://arxiv.org/abs/1902.08538},
	abstract = {Bot detection using machine learning (ML), with network ﬂow-level features, has been extensively studied in the literature. However, existing ﬂow-based approaches typically incur a high computational overhead and do not completely capture the network communication patterns, which can expose additional aspects of malicious hosts. Recently, bot detection systems which leverage communication graph analysis using ML have gained attention to overcome these limitations. A graph-based approach is rather intuitive, as graphs are true representations of network communications. In this paper, we propose a two-phased, graphbased bot detection system which leverages both unsupervised and supervised ML. The ﬁrst phase prunes presumable benign hosts, while the second phase achieves bot detection with high precision. Our system detects multiple types of bots and is robust to zero-day attacks. It also accommodates different network topologies and is suitable for large-scale data.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1902.08538 [cs]},
	author = {Daya, Abbas Abou and Salahuddin, Mohammad A. and Limam, Noura and Boutaba, Raouf},
	month = feb,
	year = {2019},
	note = {arXiv:1902.08538},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Daya et al. - 2019 - A Graph-Based Machine Learning Approach for Bot De.pdf:C\:\\Users\\charlie\\Zotero\\storage\\RH9TM4XI\\Daya et al. - 2019 - A Graph-Based Machine Learning Approach for Bot De.pdf:application/pdf}
}

@article{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1608.04644 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	note = {arXiv:1608.04644},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition},
	file = {Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:C\:\\Users\\charlie\\Zotero\\storage\\PDN3S6FR\\Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf}
}

@inproceedings{fredrikson_model_2015,
	address = {Denver, Colorado, USA},
	title = {Model {Inversion} {Attacks} that {Exploit} {Confidence} {Information} and {Basic} {Countermeasures}},
	isbn = {978-1-4503-3832-5},
	url = {http://dl.acm.org/citation.cfm?doid=2810103.2813677},
	doi = {10.1145/2810103.2813677},
	abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classiﬁers in personalized medicine by Fredrikson et al. [13], adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security} - {CCS} '15},
	publisher = {ACM Press},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	year = {2015},
	pages = {1322--1333},
	file = {Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:C\:\\Users\\charlie\\Zotero\\storage\\6A5APDC5\\Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:application/pdf}
}


@article{vadera_assessing_2020,
	title = {Assessing the {Adversarial} {Robustness} of {Monte} {Carlo} and {Distillation} {Methods} for {Deep} {Bayesian} {Neural} {Network} {Classification}},
	url = {http://arxiv.org/abs/2002.02842},
	abstract = {In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, signiﬁcantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efﬁcient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:2002.02842 [cs, stat]},
	author = {Vadera, Meet P. and Shukla, Satya Narayan and Jalaian, Brian and Marlin, Benjamin M.},
	month = feb,
	year = {2020},
	note = {arXiv:2002.02842},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2002.02842[1].pdf:C\:\\Users\\charlie\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\M03SCMDN\\2002.02842[1].pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	note = {arXiv:1610.07677},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{li_general_2016,
	title = {A {General} {Retraining} {Framework} for {Scalable} {Adversarial} {Classification}},
	url = {http://arxiv.org/abs/1604.02606},
	abstract = {Traditional classiﬁcation algorithms assume that training and test data come from similar distributions. This assumption is violated in adversarial settings, where malicious actors modify instances to evade detection. A number of custom methods have been developed for both adversarial evasion attacks and robust learning. We propose the ﬁrst systematic and general-purpose retraining framework which can: a) boost robustness of an arbitrary learning algorithm, in the face of b) a broader class of adversarial models than any prior methods. We show that, under natural conditions, the retraining framework minimizes an upper bound on optimal adversarial risk, and show how to extend this result to account for approximations of evasion attacks. Extensive experimental evaluation demonstrates that our retraining methods are nearly indistinguishable from state-of-the-art algorithms for optimizing adversarial risk, but are more general and far more scalable. The experiments also conﬁrm that without retraining, our adversarial framework dramatically reduces the effectiveness of learning. In contrast, retraining signiﬁcantly boosts robustness to evasion attacks without signiﬁcantly compromising overall accuracy.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1604.02606 [cs, stat]},
	author = {Li, Bo and Vorobeychik, Yevgeniy and Chen, Xinyun},
	month = nov,
	year = {2016},
	note = {arXiv:1604.02606},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Li et al. - 2016 - A General Retraining Framework for Scalable Advers.pdf:C\:\\Users\\charlie\\Zotero\\storage\\NGCPLE3C\\Li et al. - 2016 - A General Retraining Framework for Scalable Advers.pdf:application/pdf}
}

@article{biggio_evasion_2013,
	title = {Evasion {Attacks} against {Machine} {Learning} at {Test} {Time}},
	volume = {7908},
	url = {http://arxiv.org/abs/1708.06131},
	doi = {10.1007/978-3-642-40994-3_25},
	abstract = {In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but eﬀective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classiﬁcation algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit diﬀerent risk levels for the classiﬁer by increasing the attacker’s knowledge of the system and her ability to manipulate attack samples. This gives the classiﬁer designer a better picture of the classiﬁer performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF ﬁles, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1708.06131 [cs]},
	author = {Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Srndic, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
	year = {2013},
	note = {arXiv:1708.06131},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {387--402},
	file = {Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:C\:\\Users\\charlie\\Zotero\\storage\\KSVTHWRB\\Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:application/pdf}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	note = {arXiv:1802.01421},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	note = {arXiv:1811.11875},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	note = {arXiv:1802.05666},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}

@article{athalye_obfuscated_2018,
	title = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}: {Circumventing} {Defenses} to {Adversarial} {Examples}},
	shorttitle = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}},
	url = {http://arxiv.org/abs/1802.00420},
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimizationbased attacks, we ﬁnd defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining noncertiﬁed white-box-secure defenses at ICLR 2018, we ﬁnd obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1802.00420 [cs]},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	month = jul,
	year = {2018},
	note = {arXiv:1802.00420},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Athalye et al. - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf:C\:\\Users\\charlie\\Zotero\\storage\\36AD8ZHI\\Athalye et al. - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf:application/pdf}
}

@article{dohmatob_generalized_2019,
	title = {Generalized {No} {Free} {Lunch} {Theorem} for {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/1810.04065},
	abstract = {This manuscript presents some new impossibility results on adversarial robustness in machine learning, a very important yet largely open problem. We show that if conditioned on a class label the data distribution satisﬁes the W2 Talagrand transportation-cost inequality (for example, this condition is satisﬁed if the conditional distribution has density which is log-concave; is the uniform measure on a compact Riemannian manifold with positive Ricci curvature; etc.) any classiﬁer can be adversarially fooled with high probability once the perturbations are slightly greater than the natural noise level in the problem. We call this result The Strong ”No Free Lunch” Theorem as some recent results (Tsipras et al. 2018, Fawzi et al. 2018, etc.) on the subject can be immediately recovered as very particular cases. Our theoretical bounds are demonstrated on both simulated and real data (MNIST). We conclude the manuscript with some speculation on possible future research directions.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1810.04065 [cs, stat]},
	author = {Dohmatob, Elvis},
	month = jun,
	year = {2019},
	note = {arXiv:1810.04065},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Dohmatob - 2019 - Generalized No Free Lunch Theorem for Adversarial .pdf:C\:\\Users\\charlie\\Zotero\\storage\\59CE6T53\\Dohmatob - 2019 - Generalized No Free Lunch Theorem for Adversarial .pdf:application/pdf}
}

@article{tsipras_robustness_2019,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	url = {http://arxiv.org/abs/1805.12152},
	abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Speciﬁcally, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These ﬁndings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classiﬁers learning fundamentally different feature representations than standard classiﬁers. These differences, in particular, seem to result in unexpected beneﬁts: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1805.12152 [cs, stat]},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	month = sep,
	year = {2019},
	note = {arXiv:1805.12152},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf:C\:\\Users\\charlie\\Zotero\\storage\\2V7GJHIL\\Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf:application/pdf}
}

@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	note = {arXiv:2002.11080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}


@article{miller_adversarial_2020,
	title = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}: {A} {Comprehensive} {Review} of {Defenses} {Against} {Attacks}},
	volume = {108},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}},
	url = {https://ieeexplore.ieee.org/document/9013065/},
	doi = {10.1109/JPROC.2020.2970615},
	language = {en},
	number = {3},
	urldate = {2020-08-12},
	journal = {Proceedings of the IEEE},
	author = {Miller, David J. and Xiang, Zhen and Kesidis, George},
	month = mar,
	year = {2020},
	pages = {402--433},
	file = {Miller et al. - 2020 - Adversarial Learning Targeting Deep Neural Network.pdf:C\:\\Users\\charlie\\Zotero\\storage\\VV3I4V2C\\Miller et al. - 2020 - Adversarial Learning Targeting Deep Neural Network.pdf:application/pdf}
}

@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	note = {arXiv:1904.01555},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}



@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	number = {01},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}

@article{biggio_poisoning_2013,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	url = {http://arxiv.org/abs/1206.6389},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
	urldate = {2020-11-02},
	journal = {arXiv:1206.6389 [cs, stat]},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	month = mar,
	year = {2013},
	note = {arXiv:1206.6389},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\MXH3JBV6\\Biggio et al. - 2013 - Poisoning Attacks against Support Vector Machines.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\P4HDZ8AU\\1206.html:text/html}
}

@article{biggio_evasion_2013-1,
	title = {Evasion {Attacks} against {Machine} {Learning} at {Test} {Time}},
	volume = {7908},
	url = {http://arxiv.org/abs/1708.06131},
	doi = {10.1007/978-3-642-40994-3_25},
	abstract = {In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.},
	urldate = {2020-11-02},
	journal = {arXiv:1708.06131 [cs]},
	author = {Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Srndic, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
	year = {2013},
	note = {arXiv:1708.06131},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {387--402},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\EUR3EQFC\\Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\S3HRVLZV\\1708.html:text/html}
}

@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}


@inproceedings{deepfool,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2574--2582},
  year={2016}
}


@article{pixelattack,
  title={Adversarial Robustness Assessment: Why both $L_0$ and $L_\infty$ Attacks Are Necessary},
  author={Kotyan, Shashank and Vasconcellos Vargas, Danilo},
  journal={arXiv e-prints},
  pages={arXiv--1906},
  year={2019}
}

@article{finlayson2018adversarial,
  title={Adversarial Attacks Against Medical Deep Learning Systems},
  author={Finlayson, Samuel G and Chung, Hyung Won and Kohane, Isaac S and Beam, Andrew L},
  journal={arXiv:1804.05296},
  year={2018}
}

@inproceedings{hopskipjump,
  title={{HopSkipJumpAttack}: A query-efficient decision-based attack},
  author={Chen, Jianbo and Jordan, Michael I and Wainwright, Martin J},
  booktitle={{IEEE} symposium on security and privacy (sp)},
  pages={1277--1294},
  year={2020},
  organization={IEEE}
}

@article{fgm,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv:1412.6572},
  year={2014}
}


@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv:1706.06083},
  year={2017}
}


@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}


@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	note = {arXiv:1511.04508},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	note = {arXiv:1610.07677},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	note = {arXiv:1802.01421},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	note = {arXiv:1811.11875},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	note = {arXiv:1802.05666},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}

@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	note = {arXiv:2002.11080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}

@article{wang_security_2019,
	title = {The security of machine learning in an adversarial setting: {A} survey},
	volume = {130},
	issn = {07437315},
	shorttitle = {The security of machine learning in an adversarial setting},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518309183},
	doi = {10.1016/j.jpdc.2019.03.003},
	abstract = {Machine learning (ML) methods have demonstrated impressive performance in many application fields such as autopilot, facial recognition, and spam detection. Traditionally, ML models are trained and deployed in a benign setting, in which the testing and training data have identical statistical characteristics. However, this assumption usually does not hold in the sense that the ML model is designed in an adversarial setting, where some statistical properties of the data can be tampered with by a capable adversary. Specifically, it has been observed that adversarial examples (also known as adversarial input perambulations) elaborately crafted during training/test phases can seriously undermine the ML performance. The susceptibility of ML models in adversarial settings and the corresponding countermeasures have been studied by many researchers in both academic and industrial communities. In this work, we present a comprehensive overview of the investigation of the security properties of ML algorithms under adversarial settings. First, we analyze the ML security model to develop a blueprint for this interdisciplinary research area. Then, we review adversarial attack methods and discuss the defense strategies against them. Finally, relying upon the reviewed work, we provide prospective relevant future works for designing more secure ML models.},
	language = {en},
	urldate = {2020-08-12},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Wang, Xianmin and Li, Jing and Kuang, Xiaohui and Tan, Yu-an and Li, Jin},
	month = aug,
	year = {2019},
	pages = {12--23},
	file = {Wang et al. - 2019 - The security of machine learning in an adversarial.pdf:C\:\\Users\\charlie\\Zotero\\storage\\IZAB3YMW\\Wang et al. - 2019 - The security of machine learning in an adversarial.pdf:application/pdf}
}

@article{chakraborty_adversarial_2018,
	title = {Adversarial Attacks and Defences: {A} Survey},
	journal = {arXiv:1810.00069 [cs, stat]},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	year = {2018},
}

@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	note = {arXiv:1904.01555},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}

@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}

@article{biggio_poisoning_2013,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	url = {http://arxiv.org/abs/1206.6389},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
	urldate = {2020-11-02},
	journal = {arXiv:1206.6389 [cs, stat]},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	month = mar,
	year = {2013},
	note = {arXiv:1206.6389},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\MXH3JBV6\\Biggio et al. - 2013 - Poisoning Attacks against Support Vector Machines.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\P4HDZ8AU\\1206.html:text/html}
}

@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}

@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}

@inproceedings{biggio_multiple_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multiple {Classifier} {Systems} for {Adversarial} {Classification} {Tasks}},
	isbn = {978-3-642-02326-2},
	doi = {10.1007/978-3-642-02326-2_14},
	abstract = {Pattern classification systems are currently used in security applications like intrusion detection in computer networks, spam filtering and biometric identity recognition. These are adversarial classification problems, since the classifier faces an intelligent adversary who adaptively modifies patterns (e.g., spam e-mails) to evade it. In these tasks the goal of a classifier is to attain both a high classification accuracy and a high hardness of evasion, but this issue has not been deeply investigated yet in the literature. We address it under the viewpoint of the choice of the architecture of a multiple classifier system. We propose a measure of the hardness of evasion of a classifier architecture, and give an analytical evaluation and comparison of an individual classifier and a classifier ensemble architecture. We finally report an experimental evaluation on a spam filtering task.},
	language = {en},
	booktitle = {Multiple {Classifier} {Systems}},
	publisher = {Springer},
	author = {Biggio, Battista and Fumera, Giorgio and Roli, Fabio},
	editor = {Benediktsson, Jón Atli and Kittler, Josef and Roli, Fabio},
	year = {2009},
	pages = {132--141}
}

@article{feature_squeezing,
  title={Feature squeezing: Detecting adversarial examples in deep neural networks},
  author={Xu, Weilin and Evans, David and Qi, Yanjun},
  journal={arXiv:1704.01155},
  year={2017}
}

@inproceedings{label_smoothing,
  title={Adversarial Perturbations of Deep Neural Networks},
  author={D. Warde-Farley and I. Goodfellow},
  editor={T. Hazan , G. Papandreou, D. Tarlow},
  booktitle={Perturbations, Optimization, and Statistics},
  publisher={The MIT Press, Cambridge, Massachusetts},
  year={2017},
}

@inproceedings{tramer2016stealing,
  title={Stealing machine learning models via prediction {API}s},
  author={Tram{\`e}r, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  booktitle={25th {USENIX} Security Symposium ({USENIX} Security 16)},
  pages={601--618},
  year={2016}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}


@article{tramer,
  title={The space of transferable adversarial examples},
  author={Tram{\`e}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv:1704.03453},
  year={2017}
}
@article{papernot,
  title={The space of transferable adversarial examples},
  author={Tram{\`e}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv:1704.03453},
  year={2017}
}


@inproceedings{ruan2019global,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@article{adversarialpatch,
  title={Adversarial patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n and Gilmer, Justin},
  journal={arXiv:1712.09665},
  year={2017}
}
@inproceedings{xiao2021improving,
  title={Improving Transferability of Adversarial Patches on Face Recognition With Generative Models},
  author={Xiao, Zihao and Gao, Xianfeng and Fu, Chilin and Dong, Yinpeng and Gao, Wei and Zhang, Xiaolu and Zhou, Jun and Zhu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11845--11854},
  year={2021}
}

@article{grigorescu2020survey,
  title={A survey of deep learning techniques for autonomous driving},
  author={Grigorescu, Sorin and Trasnea, Bogdan and Cocias, Tiberiu and Macesanu, Gigel},
  journal={Journal of Field Robotics},
  volume={37},
  number={3},
  pages={362--386},
  year={2020},
  publisher={Wiley Online Library}
}

@book{nelson2010behavior,
  title={Behavior of machine learning algorithms in adversarial environments},
  author={Nelson, Blaine Alan},
  year={2010},
  publisher={University of California, Berkeley}
}


@inproceedings{al2017deep,
  title={Deep learning algorithm for autonomous driving using GoogLeNet},
  author={Al-Qizwini, Mohammed and Barjasteh, Iman and Al-Qassab, Hothaifa and Radha, Hayder},
  booktitle={2017 IEEE Intelligent Vehicles Symposium (IV)},
  pages={89--96},
  year={2017},
  organization={IEEE}
}

@INPROCEEDINGS{gauss_out, author={Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},  booktitle={2019 IEEE Symposium on Security and Privacy (SP)},   title={Certified Robustness to Adversarial Examples with Differential Privacy},   year={2019},  volume={},  number={},  pages={656-672},  doi={10.1109/SP.2019.00044}}

@ARTICLE{high_conf,  author={Chen, Li and Xiao, Jun and Zou, Pu and Li, Haifeng},  journal={IEEE Geoscience and Remote Sensing Letters},   title={Lie to Me: A Soft Threshold Defense Method for Adversarial Examples of Remote Sensing Images},   year={2021},  volume={},  number={},  pages={1-5},  doi={10.1109/LGRS.2021.3096244}}

@inproceedings{discretization,
  title={Defending against whitebox adversarial attacks via randomized discretization},
  author={Zhang, Yuchen and Liang, Percy},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={684--693},
  year={2019},
  organization={PMLR}
}


@inproceedings{tuan2010modeling,
  title={Modeling and verification of safety critical systems: A case study on pacemaker},
  author={Tuan, Luu Anh and Zheng, Man Chun and Tho, Quan Thanh},
  booktitle={2010 Fourth International Conference on Secure Software Integration and Reliability Improvement},
  pages={23--32},
  year={2010},
  organization={IEEE}
}

@inproceedings{aviation_software,
  title={Applying lessons from safety-critical systems to security-critical software},
  author={Axelrod, C Warren},
  booktitle={2011 IEEE Long Island Systems, Applications and Technology Conference},
  pages={1--6},
  year={2011},
  organization={IEEE}
}


 @book{topologytextbook, place={New York}, title={Introduction to topology}, publisher={Dover Publications}, author={Mendelson, Bert}, year={2012}} 

@inproceedings{gauss_aug,
author = {Zantedeschi, Valentina and Nicolae, Maria-Irina and Rawat, Ambrish},
title = {Efficient Defenses Against Adversarial Attacks},
year = {2017},
isbn = {9781450352024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128572.3140449},
doi = {10.1145/3128572.3140449},
abstract = {Following the recent adoption of deep neural networks (DNN) accross a wide range of
applications, adversarial attacks against these models have proven to be an indisputable
threat. Adversarial samples are crafted with a deliberate intention of undermining
a system. In the case of DNNs, the lack of better understanding of their working has
prevented the development of efficient defenses. In this paper, we propose a new defense
method based on practical observations which is easy to integrate into models and
performs better than state-of-the-art defenses. Our proposed solution is meant to
reinforce the structure of a DNN, making its prediction more stable and less likely
to be fooled by adversarial samples. We conduct an extensive experimental study proving
the efficiency of our method against multiple attacks, comparing it to numerous defenses,
both in white-box and black-box setups. Additionally, the implementation of our method
brings almost no overhead to the training procedure, while maintaining the prediction
performance of the original model on clean samples.},
booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
pages = {39–49},
numpages = {11},
keywords = {deep neural network, model security, defenses, adversarial learning},
location = {Dallas, Texas, USA},
series = {AISec '17}
}

@article{chambolle2004algorithm,
  title={An algorithm for total variation minimization and applications},
  author={Chambolle, Antonin},
  journal={Journal of Mathematical imaging and vision},
  volume={20},
  number={1},
  pages={89--97},
  year={2004},
  publisher={Springer}
}


@misc{iso26262,
    author={International Standards Organization},
    title={{ISO} 26262-1:2011, Road vehicles --- Functional safety},
    howpublished={\href{https://www.iso.org/standard/43464.html}{https://www.iso.org/standard/43464.html} (visited 2022-04-20)},
    year={2018},
}

@article{monmasson2011fpgas,
  title={FPGAs in industrial control applications},
  author={Monmasson, Eric and Idkhajine, Lahoucine and Cirstea, Marcian N and Bahri, Imene and Tisan, Alin and Naouar, Mohamed Wissem},
  journal={IEEE Transactions on Industrial informatics},
  volume={7},
  number={2},
  pages={224--243},
  year={2011},
  publisher={IEEE}
}


@article{rudin1992nonlinear,
  title={Nonlinear total variation based noise removal algorithms},
  author={Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
  journal={Physica D: nonlinear phenomena},
  volume={60},
  number={1-4},
  pages={259--268},
  year={1992},
  publisher={Elsevier}
}


@article{fukuda1992theory,
  title={Theory and applications of neural networks for industrial control systems},
  author={Fukuda, Toshio and Shibata, Takanori},
  journal={IEEE Transactions on industrial electronics},
  volume={39},
  number={6},
  pages={472--489},
  year={1992},
  publisher={IEEE}
}

@article{resnet,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  eprinttype = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reverse_sigmoid,
  author    = {Taesung Lee and
               Benjamin Edwards and
               Ian M. Molloy and
               Dong Su},
  title     = {Defending Against Model Stealing Attacks Using Deceptive Perturbations},
  journal   = {CoRR},
  volume    = {abs/1806.00054},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.00054},
  eprinttype = {arXiv},
  eprint    = {1806.00054},
  timestamp = {Wed, 02 Jun 2021 09:13:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-00054.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sahiner2019deep,
  title={Deep learning in medical imaging and radiation therapy},
  author={Sahiner, Berkman and Pezeshk, Aria and Hadjiiski, Lubomir M and Wang, Xiaosong and Drukker, Karen and Cha, Kenny H and Summers, Ronald M and Giger, Maryellen L},
  journal={Medical physics},
  volume={46},
  number={1},
  pages={e1--e36},
  year={2019},
  publisher={Wiley Online Library}
}

@book{pearson2005mining,
  title={Mining imperfect data: Dealing with contamination and incomplete records},
  author={Pearson, Ronald K},
  year={2005},
  publisher={SIAM}
}

@misc{ching2017opportunities,
  title={Opportunities and obstacles for deep learning in biology and medicine. {JR Soc Interface 15} (141)},
  author={Ching, T and Himmelstein, DS and Beaulieu-Jones, BK and Kalinin, AA and Do, BT and Way, GP and Greene, CS},
  year={2017}
}

@inproceedings{bernal2017safety++,
  title={Safety++ designing {IoT} and wearable systems for industrial safety through a user centered design approach},
  author={Bernal, Guillermo and Colombo, Sara and Al Ai Baky, Mohammed and Casalegno, Federico},
  booktitle={Proceedings of the 10th International Conference on Pervasive Technologies Related to Assistive Environments},
  pages={163--170},
  year={2017}
}


@article{pakdemirli2019artificial,
  title={Artificial intelligence in radiology: friend or foe? Where are we now and where are we heading?},
  author={Pakdemirli, Emre},
  journal={Acta radiologica open},
  volume={8},
  number={2},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}

@misc{nhtsa,
    title={Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey},
    howpublished={\href{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115}{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115} (visited 2022-04-20)},
    author={National Highway Transportation Safety Administration's (NHTSA) National Center for Statistics and Analysis},
    publisher={US Department of Transportation},
    year={2015},
}

@article{icoh, title={GLOBAL ESTIMATES OF OCCUPATIONAL  ACCIDENTS AND WORK-RELATED ILLNESSES 2017}, url={http://www.icohweb.org/site/images/news/pdf/Report\%20Global\%20Estimates\%20of\%20Occupational\%20Accidents\%20and\%20Work-related\%20Illnesses\%202017\%20rev1.pdf}, journal={International Commission on Occupational Health`}, publisher={ICOH}, author={ICOH}, year={2017}} 

@misc{OECD,
    title={{OECD} statistics},
    howpublished={\href{https://stats.oecd.org}{https://stats.oecd.org/} (visited 2022-04-20)},
    journal={{OECD} Statistics},
    publisher={International Transport Forum},
    author={The Organisation for Economic Co-operation and Development},
    year={2020},
}

@article{makary2016medical,
  title={Medical error---the third leading cause of death in the {US}},
  author={Makary, Martin A and Daniel, Michael},
  journal={{BMJ}},
  volume={353},
  year={2016},
  publisher={British Medical Journal Publishing Group}
}


@article{imageenhance,
  title={Relationship between entropy and SNR changes in image enhancement},
  author={Krbcova, Zuzana and Kukal, Jaromir},
  journal={EURASIP Journal on Image and Video Processing},
  volume={2017},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Springer}
}
@article{chung1988labelings,
  title={Labelings of graphs},
  author={Chung, Fan RK},
  journal={Selected topics in graph theory},
  volume={3},
  pages={151--168},
  year={1988}
}

@inproceedings{hamming,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@inproceedings{testautomation,
  title={Test automation for safety-critical systems: Industrial application and future developments},
  author={Peleska, Jan},
  booktitle={International Symposium of Formal Methods Europe},
  pages={39--59},
  year={1996},
  organization={Springer}
}

@book{aviation_compliance,
  title={Developing safety-critical software: a practical guide for aviation software and DO-178C compliance},
  author={Rierson, Leanna},
  year={2017},
  publisher={CRC Press}
}

@article{safetyframework,
  title={A framework for safety automation of safety-critical systems operations},
  author={Acharyulu, PV Srinivas and Seetharamaiah, P},
  journal={Safety Science},
  volume={77},
  pages={133--142},
  year={2015},
  publisher={Elsevier}
}

@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	note = {arXiv:1511.04508},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}

@article{vadera_assessing_2020,
	title = {Assessing the {Adversarial} {Robustness} of {Monte} {Carlo} and {Distillation} {Methods} for {Deep} {Bayesian} {Neural} {Network} {Classification}},
	url = {http://arxiv.org/abs/2002.02842},
	abstract = {In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, signiﬁcantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efﬁcient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:2002.02842 [cs, stat]},
	author = {Vadera, Meet P. and Shukla, Satya Narayan and Jalaian, Brian and Marlin, Benjamin M.},
	month = feb,
	year = {2020},
	note = {arXiv:2002.02842},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2002.02842[1].pdf:C\:\\Users\\charlie\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\M03SCMDN\\2002.02842[1].pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	note = {arXiv:1610.07677},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	note = {arXiv:1802.01421},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	note = {arXiv:1811.11875},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	note = {arXiv:1802.05666},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}


@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	note = {arXiv:2002.11080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}



@ARTICLE{distributed_attacks,

  author={Aljuhani, Ahamed},

  journal={IEEE Access}, 

  title={Machine Learning Approaches for Combating Distributed Denial of Service Attacks in Modern Networking Environments}, 

  year={2021},

  volume={9},

  number={},

  pages={42236-42264},

  doi={10.1109/ACCESS.2021.3062909}}



@INPROCEEDINGS{intermittent,

  author={Park, Jeman and Nyang, DaeHun and Mohaisen, Aziz},

  booktitle={2018 16th Annual Conference on Privacy, Security and Trust (PST)}, 

  title={Timing is Almost Everything: Realistic Evaluation of the Very Short Intermittent DDoS Attacks}, 

  year={2018},

  volume={},

  number={},

  pages={1-10},

  doi={10.1109/PST.2018.8514210}}





@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	note = {arXiv:1904.01555},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}

@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	number = {01},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}

@article{bishop1995training,
  title={Training with noise is equivalent to Tikhonov regularization},
  author={Bishop, Chris M},
  journal={Neural computation},
  volume={7},
  number={1},
  pages={108--116},
  year={1995},
  publisher={MIT Press}
}


@article{golub1999tikhonov,
  title={Tikhonov regularization and total least squares},
  author={Golub, Gene H and Hansen, Per Christian and O'Leary, Dianne P},
  journal={SIAM journal on matrix analysis and applications},
  volume={21},
  number={1},
  pages={185--194},
  year={1999},
  publisher={SIAM}
}

@article{koopman2016challenges,
  title={Challenges in autonomous vehicle testing and validation},
  author={Koopman, Philip and Wagner, Michael},
  journal={SAE International Journal of Transportation Safety},
  volume={4},
  number={1},
  pages={15--24},
  year={2016},
  publisher={JSTOR}
}

@article{gerth2021new,
  title={A new interpretation of (Tikhonov) regularization},
  author={Gerth, Daniel},
  journal={Inverse Problems},
  volume={37},
  number={6},
  pages={064002},
  year={2021},
  publisher={IOP Publishing}
}

@article{zhao2011modified,
  title={A modified Tikhonov regularization method for a backward heat equation},
  author={Zhao, Zhenyu and Meng, Zehong},
  journal={Inverse Problems in Science and Engineering},
  volume={19},
  number={8},
  pages={1175--1182},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{hadamard1902problemes,
  title={Sur les probl{\`e}mes aux d{\'e}riv{\'e}es partielles et leur signification physique},
  author={Hadamard, Jacques},
  journal={Princeton university bulletin},
  pages={49--52},
  year={1902},
  publisher={Princeton University}
}

@inproceedings{golub1997tikhonov,
  title={Tikhonov regularization for large scale problems},
  author={Golub, Gene H and Von Matt, Urs},
  booktitle={Workshop on scientific computing},
  pages={3--26},
  year={1997}
}



@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}

@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}

@inproceedings{ruan2019global,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}